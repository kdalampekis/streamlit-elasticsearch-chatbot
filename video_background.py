import streamlit as st
import time
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
import base64
import requests
from io import BytesIO
import numpy as np
import pandas as pd
import nltk
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
nltk.download('punkt')
import os
import re
import pdfplumber
import pytesseract
from pdf2image import convert_from_path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import ElasticsearchStore
from langchain.embeddings import HuggingFaceEmbeddings
import pytesseract, pdfplumber, tempfile, os
from pdf2image import convert_from_path
import elasticsearch
from elasticsearch import Elasticsearch


ELASTICSEARCH_HOST = "https://your_elasticsearch_url"
ELASTICSEARCH_PORT = 0****
ELASTICSEARCH_USERNAME = "your_username"  # Replace with actual username
ELASTICSEARCH_PASSWORD = "your_password"  # Replace with actual password


es_client = Elasticsearch(
    f"{ELASTICSEARCH_HOST}",
    basic_auth=(ELASTICSEARCH_USERNAME, ELASTICSEARCH_PASSWORD),
    verify_certs=False,
    request_timeout=60  # â° increase timeout

)

def ocr_from_image(image):
    return pytesseract.image_to_string(image)

def extract_text_from_pdf(file_path):
    text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
    except Exception as e:
        print(f"Error with PDFplumber: {e}. Attempting OCR fallback.")
        images = convert_from_path(file_path)
        for img in images:
            text += ocr_from_image(img)
    return text

def load_and_chunk(text, chunk_size=500, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.create_documents([text])

def process_pdf(uploaded_pdf):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_pdf.read())
        tmp_path = tmp.name

    raw_text = extract_text_from_pdf(tmp_path)
    chunks = load_and_chunk(raw_text)
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    ElasticsearchStore.from_documents(
        documents=chunks,
        embedding=embedding_model,
        index_name="your_index",
        es_connection=es_client
    )

    os.remove(tmp_path)
    print("uploaded")
    return "âœ… Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Ï€ÏÎ¿ÏƒÏ„Î­Î¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚ ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î³Î½ÏÏƒÎ·Ï‚."


# ---------------------------------------------------------
# âœ… Page config and styling
# ---------------------------------------------------------
st.set_page_config(page_title=" Multilingual Agent", page_icon="ğŸŒŠ")


def get_base64_video(video_path):
    with open(video_path, "rb") as video_file:
        return base64.b64encode(video_file.read()).decode()

def set_background_video_local(video_path):
    encoded_video = get_base64_video(video_path)
    video_tag = f"""
        <style>
        .stApp {{
            background: transparent;
        }}
        video#bgvid {{
            position: fixed;
            right: 0;
            bottom: 0;
            min-width: 100%;
            min-height: 100%;
            z-index: -1;
            object-fit: cover;
            opacity: 0.65;
        }}
        section.main > div.block-container {{
            padding-left: 350px;
            padding-right: 40px;
            max-width: 800px;
            z-index: 10;
            position: relative;
        }}
        .chat-bubble {{
            background-color: rgba(255, 255, 255, 0.95);
            padding: 1rem;
            border-radius: 1rem;
            margin: 0.5rem 0;
            font-size: 1.05rem;
            line-height: 1.6;
            font-family: "Segoe UI", sans-serif;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }}
        .user-bubble {{
            background-color: rgba(230, 247, 255, 0.95);
            padding: 1rem;
            border-radius: 1rem;
            margin: 0.5rem 0;
            font-size: 1.05rem;
            line-height: 1.6;
            font-family: "Segoe UI", sans-serif;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }}
        </style>
        <video autoplay muted loop id="bgvid">
            <source src="data:video/mp4;base64,{encoded_video}" type="video/mp4">
        </video>
    """
    st.markdown(video_tag, unsafe_allow_html=True)


set_background_video_local("your_video_background.mp4")  # replace with your local file path


# ---------------------------------------------------------
# ğŸ” Watsonx Credentials
# ---------------------------------------------------------
api_key = "your_WatsonX.ai_api_key"
project_id = "your_WatsonX.ai_project_id"
url = "your_model_deployment_url"
IAM_URL = "https://iam.cloud.ibm.com/identity/token"

# ---------------------------------------------------------
# ğŸ›ï¸ Sidebar Controls
# ---------------------------------------------------------
st.sidebar.markdown("### âš™ï¸ Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
st.sidebar.markdown("### ğŸ“„ Î‘Î½Î­Î²Î±ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ PDF")
uploaded_pdf = st.sidebar.file_uploader("Î•Ï€Î­Î»ÎµÎ¾Îµ Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î¿ PDF", type=["pdf"])
if uploaded_pdf:
    st.sidebar.success("âœ… Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ!")
    # ğŸ”œ Trigger your function here
    process_pdf(uploaded_pdf) 
decoding_method = st.sidebar.selectbox(
    "ÎœÎ­Î¸Î¿Î´Î¿Ï‚ Î‘Ï€Î¿ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚",
    ["greedy", "sample"],
    help="Î•Ï€Î­Î»ÎµÎ¾Îµ Ï„ÏÏŒÏ€Î¿ Ï€Î±ÏÎ±Î³Ï‰Î³Î®Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚: 'greedy' Î³Î¹Î± Ï€Î¹Î¿ ÏƒÎ¯Î³Î¿Ï…ÏÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚, 'sample' Î³Î¹Î± Ï€Î¹Î¿ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÎ­Ï‚."
)
temperature = st.sidebar.slider("Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒÏ„Î·Ï„Î±", 0.0, 1.5, 0.7, 0.1, help="Î•Î»Î­Î³Ï‡ÎµÎ¹ Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒÏ„Î·Ï„Î± Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…. Î§Î±Î¼Î·Î»Î­Ï‚ Ï„Î¹Î¼Î­Ï‚ â†’ Ï€Î¹Î¿ Ï€ÏÎ¿Î²Î»Î­ÏˆÎ¹Î¼ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚, Ï…ÏˆÎ·Î»Î­Ï‚ Ï„Î¹Î¼Î­Ï‚ â†’ Ï€Î¹Î¿ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î±Ï€ÏÏŒÎ²Î»ÎµÏ€Ï„ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚."
)
top_k = st.sidebar.slider("Top-k", 0, 100, 50, 5, help="Î ÎµÏÎ¹Î¿ÏÎ¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Ï€Î¹Î¸Î±Î½Î­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ Ï€Î¿Ï… ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÏƒÏ„Î¹Ï‚ top-k Ï€Î¹Î¿ Ï€Î¹Î¸Î±Î½Î­Ï‚. ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ· Ï„Î¹Î¼Î® ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ Ï€Î¹Î¿ Ï€ÏÎ¿Î²Î»Î­ÏˆÎ¹Î¼ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚."
)
top_p = st.sidebar.slider("Top-p", 0.0, 1.0, 0.9, 0.05, help="Î•Î½ÎµÏÎ³Î¿Ï€Î¿Î¹ÎµÎ¯ Î´ÎµÎ¹Î³Î¼Î±Ï„Î¿Î»Î·ÏˆÎ¯Î± Ï€Ï…ÏÎ®Î½Î± (nucleus sampling), ÎµÏ€Î¹Î»Î­Î³Î¿Î½Ï„Î±Ï‚ Ï„Î¹Ï‚ Ï€Î¹Î¿ Ï€Î¹Î¸Î±Î½Î­Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ Î¼Î­Ï‡ÏÎ¹ Î½Î± ÎºÎ±Î»Ï…Ï†Î¸ÎµÎ¯ Î· Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± p. Î .Ï‡., 0.9 ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î»Î±Î¼Î²Î¬Î½ÎµÏ„Î±Î¹ Ï…Ï€ÏŒÏˆÎ· Ï„Î¿ 90% Ï„Ï‰Î½ Ï€Î¹Î¿ Ï€Î¹Î¸Î±Î½ÏÎ½ Î»Î­Î¾ÎµÏ‰Î½."
)
max_tokens = st.sidebar.slider("ÎœÎ­Î³Î¹ÏƒÏ„Î¿Ï‚ Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î›Î­Î¾ÎµÏ‰Î½", 100, 2000, 1400, 100, help="ÎšÎ±Î¸Î¿ÏÎ¯Î¶ÎµÎ¹ Ï„Î¿ Î¼Î­Î³Î¹ÏƒÏ„Î¿ Î¼Î®ÎºÎ¿Ï‚ Ï„Î·Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Ï€Î¿Ï… Î¸Î± Ï€Î±ÏÎ±Ï‡Î¸ÎµÎ¯. Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± tokens ÏƒÎ·Î¼Î±Î¯Î½Î¿Ï…Î½ Ï€Î¹Î¿ ÎµÎºÏ„ÎµÎ½ÎµÎ¯Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚."
)
typing_effect = st.sidebar.checkbox("Î•Ï†Î­ Ï€Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚", value=True)
if st.sidebar.button("ğŸ§¹ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚"):
    st.session_state.messages = []
language = st.sidebar.radio("Î“Î»ÏÏƒÏƒÎ± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚", options=["Î•Î»Î»Î·Î½Î¹ÎºÎ¬", "Î‘Î³Î³Î»Î¹ÎºÎ¬"])
if st.sidebar.button("Î•Î¾Î±Î³Ï‰Î³Î® Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï (TXT)"):
    history = "\n\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in st.session_state.get("messages", [])])
    st.download_button("ÎšÎ±Ï„Î­Î²Î±ÏƒÎµ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ", data=history, file_name="chat_history.txt")

# ---------------------------------------------------------
# ğŸ’¬ Embeddings and Vector Search
# ---------------------------------------------------------
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

def rag_pipeline(user_query: str) -> str:
    query_vector = embedding_model.encode(user_query).tolist()

    es_client = Elasticsearch(
        "your_elasticsearch_url",
        basic_auth=("your_username", "your_password"),
        verify_certs=False,
        request_timeout=60
    )

    response = es_client.search(
        index="your_index_name",
        knn={"field": "vector", "query_vector": query_vector, "k": 5, "num_candidates": 100}
    )
    context = "\n\n".join([hit["_source"].get("text", "") for hit in response["hits"]["hits"]])

    chat_history = ""
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            chat_history += f"\nÎ•ÏÏÏ„Î·ÏƒÎ·: {msg['content']}"
        elif msg["role"] == "assistant":
            chat_history += f"\nÎ‘Ï€Î¬Î½Ï„Î·ÏƒÎ·: {msg['content']}"

    prompt = f"""Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± Î½Î± Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÏ„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ· Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·. Î— Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼ÏŒÎ½Î¿ ÏƒÏ„Î± {language}.

{chat_history}

Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚:
{context}

Î•ÏÏÏ„Î·ÏƒÎ·: {user_query}

Instructions
    -------------------------
    - Use only the above data to formulate your answer.
    - Pay attention to numbers, statistics and names.
    - Do not include any information not present in the data.
    - If the provided data is insufficient, indicate that you cannot find the answer.
    - Respond fully in {language}.
    - Write only the new generated text
    - Use bullets in order to improve readability

Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:"""

    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    data = f"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={api_key}"
    token_res = requests.post(IAM_URL, headers=headers, data=data)
    token_res.raise_for_status()
    bearer_token = token_res.json()["access_token"]

    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {bearer_token}"
    }

    if decoding_method == "greedy":
        params = {
            "decoding_method": decoding_method,
            "repetition_penalty": 1.2,
            "max_new_tokens": max_tokens,
            "min_new_tokens": 50
        }
    else:
        params = {
            "decoding_method": decoding_method,
            "temperature": temperature,
            "top_k": top_k,
            "top_p": top_p,
            "repetition_penalty": 1.2,
            "max_new_tokens": max_tokens,
            "min_new_tokens": 50
        }

    body = {
        "input": prompt,
        "parameters": params
    }


    response = requests.post(url, headers=headers, json=body)
    response.raise_for_status()
    return response.json()["results"][0]["generated_text"]

# ---------------------------------------------------------
# ğŸ–Šï¸ Typing Effect
# ---------------------------------------------------------
def simulate_typing(response_text, delay=0.02):
    output = ""
    placeholder = st.empty()
    for char in response_text:
        output += char
        placeholder.markdown(f'<div class="chat-bubble">{output}</div>', unsafe_allow_html=True)
        time.sleep(delay)
    return output

# ---------------------------------------------------------
# ğŸ’¬ Chat UI
# ---------------------------------------------------------
st.markdown("<h1 style='text-align:center; color:white;'> Marketing Greece Assistant</h1>", unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ÎšÎ±Î»Ï‰ÏƒÎ®ÏÎ¸ÎµÏ‚!  Î•Ï€Î­Î»ÎµÎ¾Îµ Î¼Î¹Î± Î±Ï€ÏŒ Ï„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹Ï‚:"})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        bubble = "user-bubble" if message["role"] == "user" else "chat-bubble"
        st.markdown(f'<div class="{bubble}">{message["content"]}</div>', unsafe_allow_html=True)

if len(st.session_state.messages) == 1:
    queries = [
        "Î ÏÏ‰Ï„Î¿Î²Î¿Ï…Î»Î¯ÎµÏ‚ Î³Î¹Î± sustainability Ï€Î¿Ï… Î±Ï†Î¿ÏÎ¬ Ï€Î¿Î»Î¹Ï„Î¹ÏƒÎ¼ÏŒ ÎºÎ±Î¹ Î¬Ï…Î»Î· Ï€Î¿Î»Î¹Ï„Î¹ÏƒÏ„Î¹ÎºÎ® ÎºÎ»Î·ÏÎ¿Î½Î¿Î¼Î¹Î¬ ÏƒÎµ Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼Î¿ÏÏ‚",
        "TÎ¬ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î½Î± Î±Ï†Î¿ÏÎ¿ÏÎ½ ÎµÎ¹Î´Î¹ÎºÎ¬ ÎºÎ¿Î¹Î½Î¬ Ï„Î±Î¾Î¹Î´Î¹Ï‰Ï„ÏÎ½",
        "Î•Î½Î´Î¹Î±Ï†Î­ÏÎ¿Ï…ÏƒÎµÏ‚ Î´ÏÎ¬ÏƒÎµÎ¹Ï‚ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±Ï‚ Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼ÏÎ½ ÏƒÎµ niche ÎºÎ¿Î¹Î½Î¬",
        "Î¦Î­ÏÎµ Î¼Î¿Ï… cases Ï€Î¿Ï… Î½Î± Î´ÎµÎ¯Ï‡Î½Î¿Ï…Î½ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ ÏƒÎµ Ï„Î¿Ï€Î¹ÎºÏŒ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ Î³Î¹Î± Ï„Î¿Î½ Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼ÏŒ"
    ]
    for q in queries:
        if st.button(q):
            st.session_state.messages.append({"role": "user", "content": q})
            with st.chat_message("assistant"):
                with st.spinner(""):
                    st.markdown("<p style='color:white; font-weight:bold;'>Î‘Î½Î±Î¶Î·Ï„Ï Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·... â³</p>", unsafe_allow_html=True)
                    try:
                        response = rag_pipeline(q)
                        final = simulate_typing(response) if typing_effect else response
                        st.session_state.messages.append({"role": "assistant", "content": final})
                    except Exception as e:
                        st.error(f"âš ï¸ Î£Ï†Î¬Î»Î¼Î±: {e}")
                        st.session_state.messages.append({"role": "assistant", "content": f"âš ï¸ Î£Ï†Î¬Î»Î¼Î±: {e}"})

user_input = st.chat_input("Î Î»Î·ÎºÏ„ÏÎ¿Î»ÏŒÎ³Î·ÏƒÎµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ¿Ï…...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").markdown(f'<div class="user-bubble">{user_input}</div>', unsafe_allow_html=True)

    with st.chat_message("assistant"):
            st.markdown("""
                    <div style='color:white; font-size:18px; font-weight:bold;'>
                    Î‘Î½Î±Î¶Î·Ï„Ï Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·
                    <span class="dots"><span>.</span><span>.</span><span>.</span></span>
                    </div>
                    <style>
                    @keyframes blink {
                    0% { opacity: 0.2; }
                    20% { opacity: 1; }
                    100% { opacity: 0.2; }
                    }
                    .dots span {
                    animation-name: blink;
                    animation-duration: 1.4s;
                    animation-iteration-count: infinite;
                    animation-fill-mode: both;
                    }
                    .dots span:nth-child(2) {
                    animation-delay: 0.2s;
                    }
                    .dots span:nth-child(3) {
                    animation-delay: 0.4s;  
                    }
                    </style>
                    """, unsafe_allow_html=True)
            try:
                response = rag_pipeline(user_input)
                final = simulate_typing(response) if typing_effect else response
                st.session_state.messages.append({"role": "assistant", "content": final})
            except Exception as e:
                error_msg = f"âš ï¸ Î£Ï†Î¬Î»Î¼Î±: {e}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})